{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654},{"sourceId":10913944,"sourceType":"datasetVersion","datasetId":6228767}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport timm\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:35:53.474945Z","iopub.execute_input":"2025-03-01T02:35:53.475182Z","iopub.status.idle":"2025-03-01T02:35:59.544834Z","shell.execute_reply.started":"2025-03-01T02:35:53.475154Z","shell.execute_reply":"2025-03-01T02:35:59.544111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nweights_file = \"/kaggle/input/weights/keypoints_mae_resnet18_rafdb_epoch49.pth\"\nweights = torch.load(weights_file, map_location=torch.device('cpu'))  # or 'cuda' if using GPU\n\nprint(weights.keys())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:35:59.546784Z","iopub.execute_input":"2025-03-01T02:35:59.547389Z","iopub.status.idle":"2025-03-01T02:36:00.223704Z","shell.execute_reply.started":"2025-03-01T02:35:59.547349Z","shell.execute_reply":"2025-03-01T02:36:00.222844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder_weights = {k.replace(\"encoder.\", \"\"): v for k, v in weights.items() if k.startswith(\"encoder.\")}\n\nmodel = timm.create_model(\n    model_name=\"resnet18\",\n    pretrained=False,\n)\nmodel.fc = nn.Identity()\nmodel.global_pool = nn.Identity()\n\nmodel.load_state_dict(encoder_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:36:00.224729Z","iopub.execute_input":"2025-03-01T02:36:00.225007Z","iopub.status.idle":"2025-03-01T02:36:00.479766Z","shell.execute_reply.started":"2025-03-01T02:36:00.224970Z","shell.execute_reply":"2025-03-01T02:36:00.478922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 7\n\nmodel.global_pool = nn.AdaptiveAvgPool2d(1)\nmodel.fc = nn.Sequential(\n    nn.Flatten(),  # Flatten to [batch_size, 512]\n    nn.Linear(in_features=512, out_features=num_classes)  # Map to [batch_size, num_classes]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:36:00.480893Z","iopub.execute_input":"2025-03-01T02:36:00.481178Z","iopub.status.idle":"2025-03-01T02:36:00.486475Z","shell.execute_reply.started":"2025-03-01T02:36:00.481149Z","shell.execute_reply":"2025-03-01T02:36:00.485680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nimport os\nimport shutil\n\ndef install_package(package):\n    subprocess.check_call([\"pip\", \"install\", package])\n    \ndef download_file(file_id, output_name):\n    install_package(\"gdown\")\n    import gdown\n    gdown.download(id=file_id, output=output_name, quiet=False)\n\ndef unzip_file(zip_file):\n    import zipfile\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        zip_ref.extractall()\n    os.remove(zip_file)\n\ndef remove_directory(directory):\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n        \n\ndef download_fer2013():\n    download_file('1YBuZaO7morIG43trYi0qtdelYGukBNCj', 'FER2013.zip')\n    unzip_file('FER2013.zip')\n\ndef download_ferplus():\n    download_file('1LShk6tZlsdBO-DciChK7y7nivUOvTAFk', 'FERPlus.zip')\n    unzip_file('FERPlus.zip')\n    remove_directory('fer_plus/train/contempt')\n    remove_directory('fer_plus/val/contempt')\n    remove_directory('fer_plus/test/contempt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:36:00.488704Z","iopub.execute_input":"2025-03-01T02:36:00.488957Z","iopub.status.idle":"2025-03-01T02:36:00.501016Z","shell.execute_reply.started":"2025-03-01T02:36:00.488932Z","shell.execute_reply":"2025-03-01T02:36:00.500324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torch\n\nprint(\"Loading data...\")\n\ntarget_size = 100\nmean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\nbatch_size = 512\n\ndata_transforms_FER = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(target_size, scale=(0.8, 1.2)),\n        transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((target_size, target_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((target_size, target_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ])\n}\n\n\n\ndataset_name = \"rafdb\"\n\nif dataset_name == \"rafdb\":\n    # Rafdb\n    train_dataset = datasets.ImageFolder(root='/kaggle/input/raf-db-dataset/DATASET/train', transform=data_transforms_FER['train'])\n    testset = datasets.ImageFolder(root='/kaggle/input/raf-db-dataset/DATASET/test', transform=data_transforms_FER['test'])\n    seed = torch.Generator().manual_seed(64)\n    # Define the split ratios\n    train_size = int(0.8 * len(train_dataset))  # 80% for training\n    val_size = len(train_dataset) - train_size  # Remaining 20% for validation\n    # Perform the split\n    trainset, valset = random_split(train_dataset, [train_size, val_size], generator=seed)\nelif dataset_name == \"fer2013\":\n    download_fer2013()\n    # Fer2013\n    trainset = datasets.ImageFolder(\n        root='org_fer2013/train',\n        transform=data_transforms_FER['train']\n    )\n    valset = datasets.ImageFolder(\n        root='org_fer2013/val',\n        transform=data_transforms_FER['val']\n    )\n    testset = datasets.ImageFolder(\n        root='org_fer2013/test',\n        transform=data_transforms_FER['test']\n    )\nelif dataset_name == \"ferplus\":\n    download_ferplus()\n    # Ferplus\n    trainset = datasets.ImageFolder(\n        root='fer_plus/train',\n        transform=data_transforms_FER['train']\n    )\n    valset = datasets.ImageFolder(\n        root='fer_plus/val',\n        transform=data_transforms_FER['val']\n    )\n    testset = datasets.ImageFolder(\n        root='fer_plus/test',\n        transform=data_transforms_FER['test']\n    )\n    \n\ntrain_loader = torch.utils.data.DataLoader(\n    trainset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=4\n)\n\nval_loader = torch.utils.data.DataLoader(\n    valset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=4\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    testset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=4\n)\n\nprint(\"Data loading completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:36:00.502211Z","iopub.execute_input":"2025-03-01T02:36:00.502884Z","iopub.status.idle":"2025-03-01T02:36:33.026967Z","shell.execute_reply.started":"2025-03-01T02:36:00.502846Z","shell.execute_reply":"2025-03-01T02:36:33.026018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Define device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# Move model to device\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Define the learning rate scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n\n# Training parameters\nepochs = 120\n\n# Metrics storage\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\nbest_val_loss = float('inf')\n\n# Training and validation loop\nfor epoch in range(epochs):\n    # Training phase\n    model.train()\n    epoch_train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n\n    train_loader_tqdm = tqdm(train_loader, desc=\"Training\", leave=False)\n    for images, labels in train_loader_tqdm:\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Update metrics\n        epoch_train_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n\n    # Calculate training metrics for the epoch\n    train_loss = epoch_train_loss / len(train_loader)\n    train_accuracy = 100 * train_correct / train_total\n\n    train_losses.append(train_loss)\n    train_accuracies.append(train_accuracy)\n\n    # Validation phase\n    model.eval()\n    epoch_val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n\n    val_loader_tqdm = tqdm(val_loader, desc=\"Validation\", leave=False)\n    with torch.no_grad():\n        for images, labels in val_loader_tqdm:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Update metrics\n            epoch_val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    # Calculate validation metrics for the epoch\n    val_loss = epoch_val_loss / len(val_loader)\n    val_accuracy = 100 * val_correct / val_total\n\n    val_losses.append(val_loss)\n    val_accuracies.append(val_accuracy)\n\n    # Step the scheduler\n    scheduler.step(val_loss)\n\n    # Save the best model\n    if val_loss < best_val_loss:\n        torch.save(model.state_dict(), f'mae_resnet18_{dataset_name}_mask0.75_epoch{epoch}.pth')\n        best_val_loss = val_loss\n\n    # Print epoch metrics\n    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n    print(f\"  Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n    print(f\"  Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T02:36:33.028200Z","iopub.execute_input":"2025-03-01T02:36:33.028901Z","execution_failed":"2025-03-01T02:44:59.316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport numpy as np\n\n# Initialize test loss and confusion matrix storage\ntest_loss = 0.0\nall_labels = []\nall_predictions = []\n\nmodel.eval()\ntest_loader_tqdm = tqdm(test_loader, desc=\"Testing\", leave=True)\nwith torch.no_grad():\n    for images, labels in test_loader_tqdm:\n        images, labels = images.to(device), labels.to(device)\n\n        # Perform inference\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n\n        # Get predictions and true labels\n        _, predicted = torch.max(outputs, 1)\n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\n# Calculate test accuracy\ntest_accuracy = 100 * np.sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)\ntest_loss /= len(test_loader)\n\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-01T02:44:59.317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion matrix\ncm = confusion_matrix(all_labels, all_predictions)\n# Normalize the confusion matrix to display percentages\ncm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n\nif dataset_name == \"rafdb\":\n    # Define RAF-DB specific class mappings\n    index_to_class = {\n        0: \"Angry\",\n        1: \"Disgust\",\n        2: \"Fear\",\n        3: \"Happy\",\n        4: \"Neutral\",\n        5: \"Sad\",\n        6: \"Surprise\"\n    }\n    classes = [index_to_class[i] for i in range(len(index_to_class))]  # Ensure consistency\nelse:\n    # Use the default class names from the dataset\n    classes = test_loader.dataset.classes\n\n# Plot and save confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_percentage, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\nplt.title(f\"Confusion Matrix - {dataset_name} classification (%)\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.tight_layout()\nplt.savefig(\"confusion_matrix.png\")\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-01T02:44:59.317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Function to plot and save training and validation metrics side by side\ndef plot_and_save_metrics(train_losses, val_losses, train_accuracies, val_accuracies, save_dir=\"metrics_plots\"):\n    import os\n    os.makedirs(save_dir, exist_ok=True)  # Ensure the save directory exists\n\n    \n    # Create a figure for losses\n    plt.figure(figsize=(12, 5))\n\n    # Training Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label='Training Loss')\n    plt.title('Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Training Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accuracies, label='Training Accuracy')\n    plt.title('Training Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n\n\n    # Save and show the losses figure\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/training.png\")\n    plt.show()\n    plt.close()\n\n\n    \n    # Create a figure for accuracies\n    plt.figure(figsize=(12, 5))\n\n    # Validation Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(val_losses, label='Validation Loss', color='orange')\n    plt.title('Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Validation Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n    plt.title('Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.legend()\n\n    # Save and show the accuracies figure\n    plt.tight_layout()\n    plt.savefig(f\"{save_dir}/validation.png\")\n    plt.show()\n    plt.close()\n\n    print(f\"Plots saved in {save_dir}\")\n\n# Example usage\n# Assuming train_losses, val_losses, train_accuracies, and val_accuracies are populated\nplot_and_save_metrics(train_losses, val_losses, train_accuracies, val_accuracies)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-01T02:44:59.317Z"}},"outputs":[],"execution_count":null}]}